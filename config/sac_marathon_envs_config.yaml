default:
    trainer: sac
    batch_size: 128
    buffer_size: 50000
    buffer_init_steps: 0
    hidden_units: 128
    init_entcoef: 1.0
    learning_rate: 3.0e-4
    learning_rate_schedule: constant
    max_steps: 5.0e5
    memory_size: 128
    normalize: false
    num_update: 1
    train_interval: 1
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    tau: 0.005
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99


    # def __init__(self, policy, env, gamma=0.99, learning_rate=3e-4, buffer_size=50000,
    #              learning_starts=100, train_freq=1, batch_size=64,
    #              tau=0.005, ent_coef='auto', target_update_interval=1,
    #              gradient_steps=1, target_entropy='auto', action_noise=None,
    #              random_exploration=0.0, verbose=0, tensorboard_log=None,
    #              _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False,
    #              seed=None, n_cpu_tf_sess=None):            

MarathonHopper2d-v0:
    time_horizon: 300
    summary_freq: 1000
    use_recurrent: false
    normalize: true 
    num_layers: 2
    hidden_units: 64
    max_steps: 100000
    batch_size: 64
    buffer_size: 100000
    init_entcoef: 1.0
    learning_rate: 3e-4
    # learning_rate_schedule: linear
    buffer_init_steps: 100
    num_update: 32
    train_interval: 1
    # reward_signals:
    #     extrinsic:
    #         strength: 1.0
    #         gamma: 0.995

    # normalize: true
    # time_horizon: 1000
    # batch_size: 256
    # train_interval: 2
    # buffer_size: 500000
    # buffer_init_steps: 2000
    # max_steps: 5e6
    # summary_freq: 30000
    # init_entcoef: 1.0
    # num_layers: 3
    # hidden_units: 512
    # reward_signals:
    #     extrinsic:
    #         strength: 1.0
    #         gamma: 0.995
